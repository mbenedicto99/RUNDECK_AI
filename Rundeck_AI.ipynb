{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMD1w+aoNmbmPRolAvVIqYh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbenedicto99/RUNDECK_AI/blob/main/Rundeck_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbiEsSyvXUVs"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Lê SOMENTE arquivos .csv do diretório 'data' e gera ./app/app/ai_analysis.json.\n",
        "\n",
        "Entrada (em data/):\n",
        "  - execucoes.csv  (campos: projeto, job, exec_id, inicio, status, duracao_s, …)\n",
        "  - score.csv      (campos: exec_id, re)\n",
        "\n",
        "Saída:\n",
        "  - app/app/ai_analysis.json\n",
        "\n",
        "Uso:\n",
        "  python scripts/build_ai_json.py \\\n",
        "    --data-dir data \\\n",
        "    --out app/app/ai_analysis.json\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import argparse\n",
        "import json\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def _fail(msg: str, code: int = 2):\n",
        "    print(f\"ERRO: {msg}\", file=sys.stderr)\n",
        "    sys.exit(code)\n",
        "\n",
        "\n",
        "def load_csvs(data_dir: Path) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Carrega execucoes.csv e score.csv exclusivamente de data_dir.\"\"\"\n",
        "    if not data_dir.exists():\n",
        "        _fail(f\"Diretório não encontrado: {data_dir}\")\n",
        "\n",
        "    # Somente .csv permitidos\n",
        "    extra = [p for p in data_dir.iterdir() if p.is_file() and p.suffix.lower() != \".csv\"]\n",
        "    if extra:\n",
        "        _fail(f\"Somente .csv são aceitos em {data_dir}. Remova/ou mova: \"\n",
        "              + \", \".join(sorted(p.name for p in extra)))\n",
        "\n",
        "    exec_path = data_dir / \"execucoes.csv\"\n",
        "    score_path = data_dir / \"score.csv\"\n",
        "\n",
        "    if not exec_path.exists():\n",
        "        _fail(f\"Arquivo obrigatório não encontrado: {exec_path}\")\n",
        "    if not score_path.exists():\n",
        "        _fail(f\"Arquivo obrigatório não encontrado: {score_path}\")\n",
        "\n",
        "    # Leitura robusta\n",
        "    df_exec = pd.read_csv(exec_path, dtype=str, keep_default_na=False, na_values=[\"\", \"NA\", \"NaN\"])\n",
        "    df_score = pd.read_csv(score_path, dtype=str, keep_default_na=False, na_values=[\"\", \"NA\", \"NaN\"])\n",
        "\n",
        "    # Tipagens mínimas / normalização\n",
        "    # exec\n",
        "    for col in [\"projeto\", \"job\", \"exec_id\", \"inicio\", \"status\", \"duracao_s\"]:\n",
        "        if col not in df_exec.columns:\n",
        "            _fail(f\"Coluna obrigatória ausente em execucoes.csv: {col}\")\n",
        "\n",
        "    # cast duracao_s -> float\n",
        "    df_exec[\"duracao_s\"] = pd.to_numeric(df_exec[\"duracao_s\"], errors=\"coerce\")\n",
        "    # parse datetime se possível\n",
        "    if \"inicio\" in df_exec.columns:\n",
        "        try:\n",
        "            df_exec[\"inicio\"] = pd.to_datetime(df_exec[\"inicio\"], errors=\"coerce\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # score\n",
        "    if \"exec_id\" not in df_score.columns or \"re\" not in df_score.columns:\n",
        "        _fail(\"Colunas obrigatórias ausentes em score.csv: exec_id, re\")\n",
        "    df_score[\"re\"] = pd.to_numeric(df_score[\"re\"], errors=\"coerce\")\n",
        "\n",
        "    # limpar ids\n",
        "    df_exec[\"exec_id\"] = df_exec[\"exec_id\"].astype(str).str.strip()\n",
        "    df_score[\"exec_id\"] = df_score[\"exec_id\"].astype(str).str.strip()\n",
        "\n",
        "    # descartar linhas inválidas\n",
        "    df_exec = df_exec[df_exec[\"exec_id\"].notna() & (df_exec[\"exec_id\"] != \"\")]\n",
        "    df_score = df_score[df_score[\"exec_id\"].notna() & (df_score[\"exec_id\"] != \"\")]\n",
        "    df_score = df_score[df_score[\"re\"].notna()]\n",
        "\n",
        "    return df_exec, df_score\n",
        "\n",
        "\n",
        "def build_analysis(df_exec: pd.DataFrame, df_score: pd.DataFrame) -> dict:\n",
        "    \"\"\"Gera dicionário com resumo, hotspots, p95 por job e amostras.\"\"\"\n",
        "    # join por exec_id\n",
        "    df = df_exec.merge(df_score[[\"exec_id\", \"re\"]], on=\"exec_id\", how=\"left\")\n",
        "\n",
        "    # métricas básicas\n",
        "    total = len(df)\n",
        "    por_status = df[\"status\"].fillna(\"desconhecido\").value_counts(dropna=False).to_dict()\n",
        "    duracao_med = float(np.nanmean(df[\"duracao_s\"])) if \"duracao_s\" in df else None\n",
        "    re_p95_global = float(np.nanpercentile(df[\"re\"], 95)) if df[\"re\"].notna().any() else None\n",
        "\n",
        "    resumo = {\n",
        "        \"total_execucoes\": int(total),\n",
        "        \"por_status\": por_status,\n",
        "        \"duracao_media_s\": None if np.isnan(duracao_med) else duracao_med,\n",
        "        \"re_p95_global\": re_p95_global,\n",
        "    }\n",
        "\n",
        "    # risco p95 por job (usando 'projeto' + 'job' quando existirem)\n",
        "    chave_job = [\"projeto\", \"job\"] if all(c in df.columns for c in [\"projeto\", \"job\"]) else [\"job\"]\n",
        "    gb = df.dropna(subset=[\"re\"]).groupby(chave_job)[\"re\"]\n",
        "    risco_p95_por_job = (\n",
        "        gb.quantile(0.95)\n",
        "        .reset_index()\n",
        "        .rename(columns={\"re\": \"re_p95\"})\n",
        "        .sort_values(\"re_p95\", ascending=False)\n",
        "        .head(200)\n",
        "        .to_dict(orient=\"records\")\n",
        "    )\n",
        "\n",
        "    # hotspots: top execuções por re\n",
        "    hotspots = (\n",
        "        df.dropna(subset=[\"re\"])\n",
        "          .sort_values(\"re\", ascending=False)\n",
        "          .loc[:, [\"projeto\", \"job\", \"exec_id\", \"inicio\", \"status\", \"duracao_s\", \"re\"]]\n",
        "          .head(50)\n",
        "    )\n",
        "    # serialização amigável\n",
        "    def _ser(dt):\n",
        "        if pd.isna(dt):\n",
        "            return None\n",
        "        if hasattr(dt, \"isoformat\"):\n",
        "            return dt.isoformat()\n",
        "        return dt\n",
        "\n",
        "    hotspots_serial = [\n",
        "        {\n",
        "            \"projeto\": r.get(\"projeto\"),\n",
        "            \"job\": r.get(\"job\"),\n",
        "            \"exec_id\": r.get(\"exec_id\"),\n",
        "            \"inicio\": _ser(r.get(\"inicio\")),\n",
        "            \"status\": r.get(\"status\"),\n",
        "            \"duracao_s\": None if pd.isna(r.get(\"duracao_s\")) else float(r.get(\"duracao_s\")),\n",
        "            \"re\": float(r.get(\"re\")),\n",
        "        }\n",
        "        for r in hotspots.to_dict(orient=\"records\")\n",
        "    ]\n",
        "\n",
        "    # amostras (curto): top 100\n",
        "    top_amostras = hotspots_serial[:100]\n",
        "\n",
        "    return {\n",
        "        \"resumo\": resumo,\n",
        "        \"risco_p95_por_job\": risco_p95_por_job,\n",
        "        \"hotspots\": hotspots_serial,\n",
        "        \"top_amostras\": top_amostras,\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"Gera ai_analysis.json a partir de .csv em 'data/'.\")\n",
        "    ap.add_argument(\"--data-dir\", default=\"data\", help=\"Diretório de entrada (somente .csv).\")\n",
        "    ap.add_argument(\"--out\", default=\"app/app/ai_analysis.json\", help=\"Arquivo de saída JSON.\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    data_dir = Path(args.data_dir)\n",
        "    out_path = Path(args.out)\n",
        "\n",
        "    df_exec, df_score = load_csvs(data_dir)\n",
        "    result = build_analysis(df_exec, df_score)\n",
        "\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(json.dumps({\n",
        "        \"status\": \"ok\",\n",
        "        \"out\": str(out_path),\n",
        "        \"resumo\": result[\"resumo\"],\n",
        "        \"counts\": {\n",
        "            \"hotspots\": len(result[\"hotspots\"]),\n",
        "            \"risco_p95_por_job\": len(result[\"risco_p95_por_job\"]),\n",
        "            \"top_amostras\": len(result[\"top_amostras\"])\n",
        "        }\n",
        "    }, ensure_ascii=False))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}