{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4lUAX+Vv+tSZnOCylSb+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbenedicto99/RUNDECK_AI/blob/main/Rundeck_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# build_ai_json.py"
      ],
      "metadata": {
        "id": "naM1eLdUcBOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Gera ai_analysis.json a partir de:\n",
        "  data/execucoes.csv (projeto, job, exec_id, inicio, status, duracao_s)\n",
        "  data/score.csv     (exec_id, re)\n",
        "\"\"\"\n",
        "\n",
        "import argparse, json, sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def _fail(msg: str, code: int = 2):\n",
        "    print(f\"ERRO: {msg}\", file=sys.stderr)\n",
        "    sys.exit(code)\n",
        "\n",
        "def _read_execucoes(exec_path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(exec_path, dtype=str, keep_default_na=False, na_values=[\"\", \"NA\", \"NaN\"])\n",
        "    # normaliza cabeçalhos comuns (aliases) -> nomes esperados\n",
        "    lower = {c.lower().strip(): c for c in df.columns}\n",
        "    rename = {}\n",
        "    if \"project\" in lower:      rename[lower[\"project\"]]      = \"projeto\"\n",
        "    if \"job_name\" in lower:     rename[lower[\"job_name\"]]     = \"job\"\n",
        "    if \"start_time\" in lower:   rename[lower[\"start_time\"]]   = \"inicio\"\n",
        "    if \"duration_sec\" in lower: rename[lower[\"duration_sec\"]] = \"duracao_s\"\n",
        "    df = df.rename(columns=rename)\n",
        "\n",
        "    required = [\"projeto\", \"job\", \"exec_id\", \"inicio\", \"status\", \"duracao_s\"]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        _fail(f\"Coluna obrigatória ausente em execucoes.csv: {', '.join(missing)}\")\n",
        "\n",
        "    # tipos mínimos\n",
        "    df[\"duracao_s\"] = pd.to_numeric(df[\"duracao_s\"], errors=\"coerce\")\n",
        "    try:\n",
        "        df[\"inicio\"] = pd.to_datetime(df[\"inicio\"], errors=\"coerce\", dayfirst=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # limpeza\n",
        "    df[\"exec_id\"] = df[\"exec_id\"].astype(str).str.strip()\n",
        "    df = df[df[\"exec_id\"].notna() & (df[\"exec_id\"] != \"\")]\n",
        "    return df\n",
        "\n",
        "def _read_score(score_path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(score_path, dtype=str, keep_default_na=False, na_values=[\"\", \"NA\", \"NaN\"])\n",
        "    if \"exec_id\" not in df.columns or \"re\" not in df.columns:\n",
        "        _fail(\"Colunas obrigatórias ausentes em score.csv: exec_id, re\")\n",
        "    df[\"exec_id\"] = df[\"exec_id\"].astype(str).str.strip()\n",
        "    df[\"re\"] = pd.to_numeric(df[\"re\"], errors=\"coerce\")\n",
        "    df = df[df[\"exec_id\"].notna() & (df[\"exec_id\"] != \"\")]\n",
        "    df = df[df[\"re\"].notna()]\n",
        "    return df\n",
        "\n",
        "def build_analysis(df_exec: pd.DataFrame, df_score: pd.DataFrame) -> dict:\n",
        "    df = df_exec.merge(df_score[[\"exec_id\", \"re\"]], on=\"exec_id\", how=\"left\")\n",
        "\n",
        "    total = len(df)\n",
        "    por_status = df[\"status\"].fillna(\"desconhecido\").value_counts(dropna=False).to_dict()\n",
        "    duracao_med = float(np.nanmean(df[\"duracao_s\"])) if \"duracao_s\" in df else None\n",
        "    re_p95_global = float(np.nanpercentile(df[\"re\"], 95)) if df[\"re\"].notna().any() else None\n",
        "\n",
        "    resumo = {\n",
        "        \"total_execucoes\": int(total),\n",
        "        \"por_status\": por_status,\n",
        "        \"duracao_media_s\": None if (duracao_med is None or np.isnan(duracao_med)) else duracao_med,\n",
        "        \"re_p95_global\": re_p95_global,\n",
        "    }\n",
        "\n",
        "    chave_job = [\"projeto\",\"job\"] if all(c in df.columns for c in [\"projeto\",\"job\"]) else [\"job\"]\n",
        "    risco_p95_por_job = (\n",
        "        df.dropna(subset=[\"re\"]).groupby(chave_job)[\"re\"].quantile(0.95)\n",
        "          .reset_index().rename(columns={\"re\":\"re_p95\"})\n",
        "          .sort_values(\"re_p95\", ascending=False).head(200)\n",
        "          .to_dict(orient=\"records\")\n",
        "    )\n",
        "\n",
        "    def _ser(v):\n",
        "        if pd.isna(v): return None\n",
        "        return v.isoformat() if hasattr(v,\"isoformat\") else v\n",
        "\n",
        "    hotspots = (\n",
        "        df.dropna(subset=[\"re\"])\n",
        "          .sort_values(\"re\", ascending=False)\n",
        "          .loc[:, [\"projeto\",\"job\",\"exec_id\",\"inicio\",\"status\",\"duracao_s\",\"re\"]]\n",
        "          .head(50).to_dict(orient=\"records\")\n",
        "    )\n",
        "    hotspots = [\n",
        "        {\"projeto\":h.get(\"projeto\"), \"job\":h.get(\"job\"), \"exec_id\":h.get(\"exec_id\"),\n",
        "         \"inicio\":_ser(h.get(\"inicio\")), \"status\":h.get(\"status\"),\n",
        "         \"duracao_s\": None if pd.isna(h.get(\"duracao_s\")) else float(h.get(\"duracao_s\")),\n",
        "         \"re\": float(h.get(\"re\"))}\n",
        "        for h in hotspots\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"resumo\": resumo,\n",
        "        \"risco_p95_por_job\": risco_p95_por_job,\n",
        "        \"hotspots\": hotspots,\n",
        "        \"top_amostras\": hotspots[:100],\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"Gera ai_analysis.json a partir de .csv em 'data/'.\")\n",
        "    ap.add_argument(\"--data-dir\", default=\"data\")\n",
        "    ap.add_argument(\"--out\", default=\"app/ai_analysis.json\")  # padrão canônico\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    data_dir = Path(args.data_dir)\n",
        "    exec_path = data_dir / \"execucoes.csv\"\n",
        "    score_path = data_dir / \"score.csv\"\n",
        "\n",
        "    if not exec_path.exists():\n",
        "        _fail(f\"Arquivo obrigatório não encontrado: {exec_path}\")\n",
        "    if not score_path.exists():\n",
        "        _fail(f\"Arquivo obrigatório não encontrado: {score_path}\")\n",
        "\n",
        "    df_exec = _read_execucoes(exec_path)\n",
        "    df_score = _read_score(score_path)\n",
        "\n",
        "    result = build_analysis(df_exec, df_score)\n",
        "\n",
        "    out_path = Path(args.out)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(json.dumps({\n",
        "        \"status\": \"ok\",\n",
        "        \"out\": str(out_path),\n",
        "        \"resumo\": result[\"resumo\"],\n",
        "        \"counts\": {\n",
        "            \"hotspots\": len(result[\"hotspots\"]),\n",
        "            \"risco_p95_por_job\": len(result[\"risco_p95_por_job\"]),\n",
        "            \"top_amostras\": len(result[\"top_amostras\"])\n",
        "        }\n",
        "    }, ensure_ascii=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "YB0oEjVImC5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# etl.py"
      ],
      "metadata": {
        "id": "MPUZN03Qb0YD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dateutil import parser\n",
        "\n",
        "# Entradas/Saídas\n",
        "INPUT_FILE   = os.getenv(\"INPUT_CSV\", \"data/slice.txt\")   # pode ser .txt ou .csv\n",
        "CLEAN_CSV    = os.getenv(\"OUTPUT_CSV\", \"data/clean.csv\")\n",
        "EXECUCOES_CSV = os.getenv(\"EXECUCOES_CSV\", \"data/execucoes.csv\")\n",
        "\n",
        "def _ensure_exists(p: str | Path):\n",
        "    if not Path(p).exists():\n",
        "        raise FileNotFoundError(f\"Arquivo não encontrado: {p}\")\n",
        "\n",
        "def _try_read(path: str | Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Lê CSV/TXT com robustez:\n",
        "      1) tenta ; (slice.txt geralmente vem com ;) e BOM\n",
        "      2) tenta , como fallback\n",
        "    \"\"\"\n",
        "    # tentativa 1: ;\n",
        "    try:\n",
        "        return pd.read_csv(path, sep=';', encoding='utf-8-sig', quotechar='\"', engine='python')\n",
        "    except Exception:\n",
        "        pass\n",
        "    # tentativa 2: ,\n",
        "    return pd.read_csv(path, sep=',', encoding='utf-8-sig', quotechar='\"', engine='python')\n",
        "\n",
        "def _parse_dt(x):\n",
        "    if pd.isna(x) or x is None:\n",
        "        return pd.NaT\n",
        "    try:\n",
        "        # datas pt-BR: dayfirst=True\n",
        "        return parser.parse(str(x), dayfirst=True)\n",
        "    except Exception:\n",
        "        return pd.NaT\n",
        "\n",
        "def _norm_status(s: pd.Series) -> pd.Series:\n",
        "    st = s.astype(str).str.lower().str.strip()\n",
        "    return st.replace({\n",
        "        \"succeeded\": \"success\",\n",
        "        \"succeed\":   \"success\",\n",
        "        \"successful\":\"success\",\n",
        "        \"ok\":        \"success\",\n",
        "        \"pass\":      \"success\",\n",
        "        \"completed\": \"success\",\n",
        "        \"done\":      \"success\",\n",
        "        \"fail\":      \"failed\",\n",
        "        \"failed\":    \"failed\",\n",
        "        \"error\":     \"failed\",\n",
        "        \"ko\":        \"failed\"\n",
        "    })\n",
        "\n",
        "def _hash_exec_id(proj: str, job: str, inicio) -> str:\n",
        "    base = f\"{proj}|{job}|{inicio}\"\n",
        "    return hashlib.sha1(base.encode(\"utf-8\"), usedforsecurity=False).hexdigest()[:16]\n",
        "\n",
        "def main():\n",
        "    _ensure_exists(INPUT_FILE)\n",
        "\n",
        "    df_raw = _try_read(INPUT_FILE)\n",
        "    if df_raw.empty:\n",
        "        raise ValueError(f\"{INPUT_FILE} lido mas sem linhas.\")\n",
        "\n",
        "    # normaliza cabeçalhos\n",
        "    df_raw.columns = [c.strip().lower() for c in df_raw.columns]\n",
        "\n",
        "    # mapeia possíveis nomes (aliases) vindos do slice\n",
        "    # ex.: \"Ended Status\", \"Start Time\", \"End Time\", \"Application\", \"Sub-Application\"\n",
        "    colmap = {\n",
        "        \"job_id\":     [\"job_id\", \"id\", \"execution_id\"],\n",
        "        \"job\":        [\"job\", \"job_name\", \"name\", \"application\"],\n",
        "        \"projeto\":    [\"projeto\", \"project\", \"project_name\", \"sub-application\", \"folder\"],\n",
        "        \"status\":     [\"status\", \"result\", \"state\", \"ended status\"],\n",
        "        \"inicio\":     [\"inicio\", \"start_time\", \"started_at\", \"start\", \"start time\"],\n",
        "        \"fim\":        [\"fim\", \"end_time\", \"ended_at\", \"end\", \"finish_time\", \"end time\"],\n",
        "    }\n",
        "\n",
        "    def pick(keys):\n",
        "        for k in keys:\n",
        "            if k in df_raw.columns:\n",
        "                return df_raw[k]\n",
        "        return pd.Series([None] * len(df_raw))\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"job_id\":  pick(colmap[\"job_id\"]),\n",
        "        \"job\":     pick(colmap[\"job\"]),\n",
        "        \"projeto\": pick(colmap[\"projeto\"]),\n",
        "        \"status\":  pick(colmap[\"status\"]),\n",
        "        \"inicio\":  pick(colmap[\"inicio\"]),\n",
        "        \"fim\":     pick(colmap[\"fim\"]),\n",
        "    })\n",
        "\n",
        "    # parsing de datas e duração\n",
        "    df[\"inicio\"] = df[\"inicio\"].apply(_parse_dt)\n",
        "    df[\"fim\"]    = df[\"fim\"].apply(_parse_dt)\n",
        "    df[\"duration_sec\"] = (df[\"fim\"] - df[\"inicio\"]).dt.total_seconds()\n",
        "\n",
        "    # normalização de status + saneamento\n",
        "    df[\"status\"] = _norm_status(df[\"status\"])\n",
        "    df = df.dropna(subset=[\"inicio\"])\n",
        "    df[\"duration_sec\"] = pd.to_numeric(df[\"duration_sec\"], errors=\"coerce\").fillna(0.0).clip(lower=0.0)\n",
        "\n",
        "    # defaults para texto\n",
        "    df[\"job\"] = df[\"job\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
        "    df[\"projeto\"] = df[\"projeto\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
        "\n",
        "    # derivações de tempo\n",
        "    df[\"date\"]    = df[\"inicio\"].dt.date\n",
        "    df[\"hour\"]    = df[\"inicio\"].dt.hour\n",
        "    df[\"weekday\"] = df[\"inicio\"].dt.weekday\n",
        "\n",
        "    # exec_id: usa job_id se existir; senão hash determinístico de projeto|job|inicio ISO\n",
        "    if df[\"job_id\"].notna().any():\n",
        "        df[\"exec_id\"] = df[\"job_id\"].astype(str).str.strip()\n",
        "    else:\n",
        "        df[\"exec_id\"] = df.apply(lambda r: _hash_exec_id(r[\"projeto\"], r[\"job\"], getattr(r[\"inicio\"], \"isoformat\", lambda: r[\"inicio\"])()), axis=1)\n",
        "\n",
        "    # ordena por inicio (opcional)\n",
        "    df = df.sort_values(\"inicio\").reset_index(drop=True)\n",
        "\n",
        "    # salva clean.csv (mantém colunas úteis ao features.py)\n",
        "    cols_clean = [\n",
        "        \"projeto\", \"job\", \"exec_id\", \"inicio\", \"fim\", \"status\",\n",
        "        \"duration_sec\", \"date\", \"hour\", \"weekday\"\n",
        "    ]\n",
        "    Path(CLEAN_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
        "    df[cols_clean].to_csv(CLEAN_CSV, index=False)\n",
        "    print(f\"[etl] Gravado {CLEAN_CSV} com {len(df)} linhas.\")\n",
        "\n",
        "    # salva execucoes.csv (layout esperado pelo build_ai_json.py)\n",
        "    execucoes = df.rename(columns={\n",
        "        \"inicio\": \"inicio\",\n",
        "        \"duration_sec\": \"duracao_s\"\n",
        "    })[[\"projeto\", \"job\", \"exec_id\", \"inicio\", \"status\", \"duracao_s\"]]\n",
        "\n",
        "    Path(EXECUCOES_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
        "    execucoes.to_csv(EXECUCOES_CSV, index=False)\n",
        "    print(f\"[etl] Gravado {EXECUCOES_CSV} com {len(execucoes)} linhas.\")\n",
        "\n",
        "    # diagnóstico rápido\n",
        "    print(\"[etl] Amostra clean.csv:\")\n",
        "    print(df[cols_clean].head(3).to_string(index=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "q1so7FOCb40J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pipeline.py"
      ],
      "metadata": {
        "id": "8miQX_0lcvkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "STEPS = [\n",
        "    [\"python\", \"scripts/etl.py\"],\n",
        "    [\"python\", \"scripts/features.py\"],\n",
        "    [\"python\", \"scripts/train_rbm.py\"],\n",
        "    [\"python\", \"scripts/detect_anomalies.py\"],  # gera data/score.csv e (opcional) app/ai_analysis.json\n",
        "    [\"python\", \"scripts/build_ai_json.py\", \"--data-dir\", \"data\", \"--out\", \"app/ai_analysis.json\"],\n",
        "]\n",
        "\n",
        "def run_step(cmd):\n",
        "    print(f\"[pipeline] Executando: {' '.join(cmd)}\", flush=True)\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"[pipeline] Erro no passo: {' '.join(cmd)}\", file=sys.stderr, flush=True)\n",
        "        sys.exit(e.returncode)\n",
        "\n",
        "def main():\n",
        "    Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(\"models\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(\"app\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for step in STEPS:\n",
        "        run_step(step)\n",
        "\n",
        "    out_path = Path(\"app/ai_analysis.json\").resolve()\n",
        "    print(f\"[pipeline] ✔ Finalizado com sucesso.\\n[pipeline] Saída: {out_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "IitbPx0Gcpxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# detect_anomalies.py"
      ],
      "metadata": {
        "id": "h9wLGXE5f5Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "# Entradas/Saídas (podem ser sobrescritas por env vars)\n",
        "FEATS_CSV  = os.getenv(\"FEATS_CSV\", \"data/features.csv\")\n",
        "SCALER_JOB = os.getenv(\"SCALER_JOB\", \"models/scalers.joblib\")  # salvo no train_rbm.py\n",
        "RBM_JOB    = os.getenv(\"RBM_JOB\", \"models/rbm.joblib\")\n",
        "\n",
        "SCORE_CSV  = os.getenv(\"SCORE_CSV\", \"data/score.csv\")          # <- requerido pelo build_ai_json.py\n",
        "OUT_JSON   = os.getenv(\"OUT_JSON\", \"app/ai_analysis.json\")      # <- caminho canônico do painel\n",
        "\n",
        "def _ensure_exists(path: str | Path, kind: str):\n",
        "    if not Path(path).exists():\n",
        "        raise FileNotFoundError(f\"{kind} não encontrado: {path}\")\n",
        "\n",
        "def _load_inputs():\n",
        "    _ensure_exists(FEATS_CSV, \"CSV de features\")\n",
        "    _ensure_exists(SCALER_JOB, \"Scaler/metadata\")\n",
        "    _ensure_exists(RBM_JOB, \"Modelo RBM\")\n",
        "    df = pd.read_csv(FEATS_CSV)\n",
        "    meta = joblib.load(SCALER_JOB)\n",
        "    rbm  = joblib.load(RBM_JOB)\n",
        "\n",
        "    used_cols = meta.get(\"used_cols\")\n",
        "    scaler    = meta.get(\"scaler\")\n",
        "    if used_cols is None or scaler is None:\n",
        "        raise ValueError(\"models/scalers.joblib não possui 'used_cols' e/ou 'scaler'.\")\n",
        "\n",
        "    if not all(c in df.columns for c in used_cols):\n",
        "        faltando = [c for c in used_cols if c not in df.columns]\n",
        "        raise ValueError(f\"Colunas de features ausentes no features.csv: {faltando}\")\n",
        "\n",
        "    return df, used_cols, scaler, rbm\n",
        "\n",
        "def _prepare_matrix(df: pd.DataFrame, used_cols, scaler):\n",
        "    X = df[used_cols].copy()\n",
        "    # coerção robusta pra numérico\n",
        "    for c in X.columns:\n",
        "        if not pd.api.types.is_numeric_dtype(X[c]):\n",
        "            # troca vírgula por ponto, tenta numérico\n",
        "            X[c] = pd.to_numeric(X[c].astype(str).str.replace(\",\", \".\", regex=False), errors=\"coerce\")\n",
        "        # imputação mediana\n",
        "        med = X[c].median(skipna=True)\n",
        "        X[c] = X[c].fillna(med)\n",
        "    # escala igual ao treino e clipa a [0,1]\n",
        "    Xn = scaler.transform(X.values.astype(np.float64))\n",
        "    Xn = np.clip(Xn, 0.0, 1.0)\n",
        "    return Xn\n",
        "\n",
        "def main():\n",
        "    df, used_cols, scaler, rbm = _load_inputs()\n",
        "\n",
        "    # Garantir identificador por linha\n",
        "    id_col = \"exec_id\" if \"exec_id\" in df.columns else None\n",
        "    if id_col is None:\n",
        "        df[\"exec_id\"] = np.arange(len(df)).astype(str)\n",
        "        id_col = \"exec_id\"\n",
        "\n",
        "    X = _prepare_matrix(df, used_cols, scaler)\n",
        "\n",
        "    # Reconstrução (usando passo de Gibbs do RBM)\n",
        "    V_recon = rbm.gibbs(X)\n",
        "    re = np.mean((X - V_recon) ** 2, axis=1)\n",
        "\n",
        "    # Salva score.csv para o build final\n",
        "    out_df = pd.DataFrame({\"exec_id\": df[id_col].astype(str), \"re\": re.astype(float)})\n",
        "    Path(SCORE_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_df.to_csv(SCORE_CSV, index=False)\n",
        "    print(f\"[detect_anomalies] Gravado {SCORE_CSV} com {len(out_df)} linhas.\")\n",
        "\n",
        "    # (opcional) JSON leve no caminho canônico; o build_ai_json.py sobrescreve depois com o layout completo\n",
        "    resumo = {\n",
        "        \"total_execucoes\": int(len(out_df)),\n",
        "        \"re_p95_global\": float(np.percentile(re, 95)) if len(out_df) else None\n",
        "    }\n",
        "    payload = {\"resumo\": resumo, \"scores_sample\": out_df.head(10).to_dict(orient=\"records\")}\n",
        "    Path(OUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"[detect_anomalies] Gravado JSON em {OUT_JSON}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bjwiPqVUj79g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# features.py"
      ],
      "metadata": {
        "id": "175MaM_4gn1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "INPUT_CLEAN = os.getenv(\"INPUT_CLEAN\", \"data/clean.csv\")\n",
        "OUTPUT_FEATS = os.getenv(\"OUTPUT_FEATS\", \"data/features.csv\")\n",
        "\n",
        "def _ensure_exists(path: str | Path):\n",
        "    if not Path(path).exists():\n",
        "        raise FileNotFoundError(f\"Arquivo não encontrado: {path}\")\n",
        "\n",
        "def _to_datetime(s: pd.Series):\n",
        "    if np.issubdtype(s.dtype, np.datetime64):\n",
        "        return s\n",
        "    return pd.to_datetime(s, errors=\"coerce\", dayfirst=True, utc=False)\n",
        "\n",
        "def _hash_exec_id(row: pd.Series) -> str:\n",
        "    base = f\"{row.get('projeto','')}|{row.get('job','')}|{row.get('inicio','')}\"\n",
        "    h = hashlib.sha1(base.encode(\"utf-8\"), usedforsecurity=False).hexdigest()[:16]\n",
        "    return h\n",
        "\n",
        "def _minmax_01(x: pd.Series) -> pd.Series:\n",
        "    v = x.astype(float)\n",
        "    vmin, vmax = np.nanmin(v.values), np.nanmax(v.values)\n",
        "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmax == vmin:\n",
        "        # tudo igual ou inválido -> vira zeros\n",
        "        return pd.Series(np.zeros(len(v)), index=v.index, dtype=float)\n",
        "    return (v - vmin) / (vmax - vmin)\n",
        "\n",
        "def _zclip_to01(x: pd.Series, clip=3.0) -> pd.Series:\n",
        "    v = x.astype(float)\n",
        "    mu = np.nanmean(v.values)\n",
        "    sd = np.nanstd(v.values)\n",
        "    if not np.isfinite(sd) or sd == 0:\n",
        "        return pd.Series(np.full(len(v), 0.5), index=v.index, dtype=float)\n",
        "    z = (v - mu) / sd\n",
        "    z = np.clip(z, -clip, clip)\n",
        "    return (z + clip) / (2 * clip)\n",
        "\n",
        "def _cyc_enc_01(vals: pd.Series, period: int):\n",
        "    # seno/cosseno em [-1,1], depois remapeia para [0,1]\n",
        "    ang = 2 * np.pi * (vals.astype(float) % period) / period\n",
        "    s = np.sin(ang)\n",
        "    c = np.cos(ang)\n",
        "    return (s + 1.0) / 2.0, (c + 1.0) / 2.0\n",
        "\n",
        "def _p95_flags_per_job(df: pd.DataFrame, dur_col=\"duration_sec\", key_cols=(\"projeto\",\"job\")) -> pd.Series:\n",
        "    tmp = df.copy()\n",
        "    tmp[dur_col] = pd.to_numeric(tmp[dur_col], errors=\"coerce\").fillna(0.0).clip(lower=0.0)\n",
        "    if all(k in tmp.columns for k in key_cols):\n",
        "        thr = tmp.groupby(list(key_cols))[dur_col].quantile(0.95)\n",
        "        thr = thr.rename(\"thr\").reset_index()\n",
        "        j = tmp[list(key_cols) + [dur_col]].merge(thr, on=list(key_cols), how=\"left\")\n",
        "        # fallback pro global se alguma chave não tiver threshold\n",
        "        global_thr = float(np.percentile(tmp[dur_col].values, 95)) if len(tmp) else np.inf\n",
        "        j[\"thr\"] = j[\"thr\"].fillna(global_thr)\n",
        "        flags = (j[dur_col] > j[\"thr\"]).astype(int)\n",
        "        flags.index = df.index\n",
        "        return flags\n",
        "    # fallback global direto\n",
        "    gthr = float(np.percentile(tmp[dur_col].values, 95)) if len(tmp) else np.inf\n",
        "    return (tmp[dur_col] > gthr).astype(int)\n",
        "\n",
        "def main():\n",
        "    _ensure_exists(INPUT_CLEAN)\n",
        "    df = pd.read_csv(INPUT_CLEAN)\n",
        "\n",
        "    # Normaliza nomes esperados pelo pipeline\n",
        "    # Esperado (do ETL ajustado): projeto, job, exec_id, inicio, status, duration_sec, date, hour, weekday\n",
        "    # Garante colunas mínimas:\n",
        "    col_alias = {c.lower(): c for c in df.columns}\n",
        "    # padroniza para lower para trabalhar\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "\n",
        "    # Mapeia possíveis nomes\n",
        "    rename_map = {}\n",
        "    if \"project\" in df.columns: rename_map[\"project\"] = \"projeto\"\n",
        "    if \"job_name\" in df.columns: rename_map[\"job_name\"] = \"job\"\n",
        "    if \"start_time\" in df.columns: rename_map[\"start_time\"] = \"inicio\"\n",
        "    if \"duration_sec\" not in df.columns and \"duracao_s\" in df.columns:\n",
        "        rename_map[\"duracao_s\"] = \"duration_sec\"\n",
        "    if rename_map:\n",
        "        df = df.rename(columns=rename_map)\n",
        "\n",
        "    # Conserta datetime e deriva hora/weekday se faltar\n",
        "    if \"inicio\" in df.columns:\n",
        "        df[\"inicio\"] = _to_datetime(df[\"inicio\"])\n",
        "        df[\"hour\"] = df[\"hour\"] if \"hour\" in df.columns else df[\"inicio\"].dt.hour\n",
        "        df[\"weekday\"] = df[\"weekday\"] if \"weekday\" in df.columns else df[\"inicio\"].dt.weekday\n",
        "    else:\n",
        "        # se não houver 'inicio', cria hora/weekday nulos\n",
        "        df[\"hour\"] = df.get(\"hour\", pd.Series([np.nan]*len(df)))\n",
        "        df[\"weekday\"] = df.get(\"weekday\", pd.Series([np.nan]*len(df)))\n",
        "\n",
        "    # duration_sec\n",
        "    if \"duration_sec\" not in df.columns:\n",
        "        raise ValueError(\"Coluna 'duration_sec' ausente em data/clean.csv (verifique o ETL).\")\n",
        "    df[\"duration_sec\"] = pd.to_numeric(df[\"duration_sec\"], errors=\"coerce\").fillna(0.0).clip(lower=0.0)\n",
        "\n",
        "    # status → failed\n",
        "    if \"status\" in df.columns:\n",
        "        st = df[\"status\"].astype(str).str.lower().str.strip()\n",
        "        failed = st.eq(\"failed\").astype(int)\n",
        "    else:\n",
        "        failed = pd.Series(np.zeros(len(df), dtype=int), index=df.index)\n",
        "\n",
        "    # projeto/job para agregações\n",
        "    if \"projeto\" not in df.columns: df[\"projeto\"] = \"UNKNOWN\"\n",
        "    if \"job\" not in df.columns: df[\"job\"] = \"UNKNOWN\"\n",
        "\n",
        "    # exec_id (prioriza campo existente; senão tenta job_id; se não houver, hash determinístico)\n",
        "    if \"exec_id\" in df.columns:\n",
        "        exec_id = df[\"exec_id\"].astype(str).fillna(\"\").str.strip()\n",
        "    elif \"job_id\" in df.columns:\n",
        "        exec_id = df[\"job_id\"].astype(str).fillna(\"\").str.strip()\n",
        "    else:\n",
        "        # cria id determinístico\n",
        "        if \"inicio\" not in df.columns:\n",
        "            df[\"inicio\"] = pd.NaT\n",
        "        exec_id = df.apply(_hash_exec_id, axis=1)\n",
        "\n",
        "    # features\n",
        "    duration_sec_mm = _minmax_01(df[\"duration_sec\"])\n",
        "    duration_z_clipped_mm = _zclip_to01(df[\"duration_sec\"], clip=3.0)\n",
        "\n",
        "    # hora e weekday (tratando NaN como 0)\n",
        "    hour = pd.to_numeric(df[\"hour\"], errors=\"coerce\").fillna(0).clip(lower=0, upper=23)\n",
        "    wday = pd.to_numeric(df[\"weekday\"], errors=\"coerce\").fillna(0).clip(lower=0, upper=6)\n",
        "\n",
        "    hour_sin_mm, hour_cos_mm = _cyc_enc_01(hour, period=24)\n",
        "    wday_sin_mm, wday_cos_mm = _cyc_enc_01(wday, period=7)\n",
        "\n",
        "    high_runtime = _p95_flags_per_job(df, dur_col=\"duration_sec\", key_cols=(\"projeto\",\"job\"))\n",
        "\n",
        "    feats = pd.DataFrame({\n",
        "        \"exec_id\": exec_id.astype(str),\n",
        "        \"duration_sec_mm\": duration_sec_mm.astype(float),\n",
        "        \"duration_z_clipped_mm\": duration_z_clipped_mm.astype(float),\n",
        "        \"hour_sin_mm\": hour_sin_mm.astype(float),\n",
        "        \"hour_cos_mm\": hour_cos_mm.astype(float),\n",
        "        \"wday_sin_mm\": wday_sin_mm.astype(float),\n",
        "        \"wday_cos_mm\": wday_cos_mm.astype(float),\n",
        "        \"failed\": failed.astype(int),\n",
        "        \"high_runtime\": high_runtime.astype(int),\n",
        "    })\n",
        "\n",
        "    # Diagnóstico rápido\n",
        "    print(\"[features] linhas:\", len(feats))\n",
        "    print(\"[features] nulos por coluna:\\n\", feats.isna().sum())\n",
        "    print(\"[features] amostra:\\n\", feats.head(3).to_string(index=False))\n",
        "\n",
        "    # Grava\n",
        "    Path(OUTPUT_FEATS).parent.mkdir(parents=True, exist_ok=True)\n",
        "    feats.to_csv(OUTPUT_FEATS, index=False)\n",
        "    print(f\"[features] Gravado {OUTPUT_FEATS} com {len(feats.columns)-1} features (+ exec_id).\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "MEKoMSEkgX6n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}